{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc3Zcd6vrdrnL5tpzmv66d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalafadli/UAS_AI_2023/blob/main/UAS_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PcytDwwiGJ1J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        " data = pd.read_csv('twitter_MBTI.csv')\n",
        " return data"
      ],
      "metadata": {
        "id": "SevM8wfwLek2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df = load_data()"
      ],
      "metadata": {
        "id": "sKCWxHsMLenf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tampilkan 5 data hasil pengambilan data dar exel\n",
        "tweet_df.head(5)"
      ],
      "metadata": {
        "id": "Qw4X-dyyLerL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mengubah urutan kolom dan dimaasukkan ke data frame agar mudah ke bagian preprosseing\n",
        "df = pd.DataFrame(tweet_df[['label','text']])"
      ],
      "metadata": {
        "id": "X57dk77cMPB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hitung total data\n",
        "df.shape"
      ],
      "metadata": {
        "id": "pgMVSZZMMD_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#menghitung banyaknya data sesuai jenis label nya\n",
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "3rxvl6HAMECg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#memunculkan data grafik berdasarkan label untuk menunjukkan keseimbangan data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "label_cnt = Counter(df.label)\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.bar(label_cnt.keys(), label_cnt.values())\n",
        "\n",
        "plt.title(\"Dataset labels distribuition\")\n",
        "\n",
        "#mengubah huruf menjadi kecil semua\n",
        "df['lower'] = df['text'].str.lower()"
      ],
      "metadata": {
        "id": "NazXiPTHMEGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re # impor modul regular expression\n",
        "\n",
        "def hapus_angka(tweet):\n",
        "  tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "  return tweet\n"
      ],
      "metadata": {
        "id": "UrAr-D1FMyqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['h_angka']= df['lower'].apply(lambda x: hapus_angka(x))"
      ],
      "metadata": {
        "id": "LdS2OfX5My3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "twsOuPwBM751"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import stopword\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_indonesia = stopwords.words('indonesian')"
      ],
      "metadata": {
        "id": "H1RTnEHZM79W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sastrawi"
      ],
      "metadata": {
        "id": "FMSOYNzENbo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import sastrawi\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "Np47D41INbsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "df-Wng8VNkMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#persiapkan untuk emoticon yang di hapus\n",
        "# all\n",
        "\n",
        "emoticons_phone = set(['🤣','😁','😂','🖖','😘','❤️','😄','😔','☺️','👍','😊','😍','😭','💋','😒','😳','😜','🙈','😡',\n",
        "\n",
        "'😱','😝','😢','😃','😉','😏','😞','😅','😚','🙊','😌','😀','😋','😆','😕','👌','😀','😃',\n",
        "\n",
        "'😄','😁','😆','😅','😂','🤣','☺️','😊','😇','🙂','🙃','😉','😌','😍','🥰','😘','😗','😙','😚',\n",
        "\n",
        "'😋','😛','😝','😜','🤪','🤨','🧐','🤓','😎','🤩','🥳','😏','😒','😞','😔','😟','😕','🙁',\n",
        "\n",
        "'☹️','😣','😖','😫','😩','🥺','😢','😭','😤','😠','😡','🤬','🤯','😳','🥵','🥶','😱','😨',\n",
        "\n",
        "'😰','😥','😓','🤗','🤔','🤭🤫','🤥😶','😐','😑','😬','🙄','😯','😦','😧','😮','😲','🥱','😴',\n",
        "\n",
        "'🤤','😪','😵','🤐','🥴','🤢','🤮','🤧','😷','🤒','🤕','🤑','🤠😈','👿','👹','👺','🤡','💩',\n",
        "\n",
        "'👻','💀','☠️','👽','👾','🤖','🎃','😺','😸','😻','😹','😼','😽','🙀','😿','😾','🤲','👍','🤝',\n",
        "\n",
        "'👎','🤜','🤛','🤞','✌️','🤟','🤘','👌','🤏','👈','👉','👆','👇','☝️','✋','🤚','🖐','🖖','👋','🤙',\n",
        "\n",
        "'💪','🦾','👇','🖕','🙏','🦶','🦵','🦿','💄','💋','👄','🦷','👅','👃','🦻','👣','👁','👀','🧠',\n",
        "\n",
        "'🗣👤','👥','👶','👧','🧒','👦','👩','🧑','👨','👩‍🦱','🧑‍🦱','👨‍🦱👩‍🦰','🧑‍🦰👨‍🦰','👱‍♀️','👱','👱‍♂️','👩‍🦳','🧑‍🦳',\n",
        "\n",
        "'👨‍🦳','👩‍🦲','🧑‍🦲','👨‍🦲','🧔','👵','🧓','👴','👲','👳‍♀️','👳','👳‍♂️','🧕','👮‍♀️','👲','👮‍♂️','👷‍♀️','👷','👷‍♂️',\n",
        "\n",
        "'💂‍♀️','💂','💂‍♂️','🕵️‍♀️','🕵️','🕵️‍♂️','👩‍⚕️','🧑‍⚕️','👨‍⚕️','👩‍🌾','🐒','🙊','🐵','🙈','🙉','🐷','🐻','🐶','🐱',\n",
        "\n",
        "'🦊','🚗','🚕','🚙','🚌','🚎','🏎','🚛','🚚','🚐','🚒','🚑','🚓','🏢','🏠','🏡','🏬','🏘','🏣',\n",
        "\n",
        "'🏚','🏤','🏗','🏥','⛺️','🏦','🏛','💒','🏩','🏫','🏪','🏨','⛪️','🕌','🕍','🛕','🕋','⛩','📈',\n",
        "\n",
        "'📉','🗒','🗓'])"
      ],
      "metadata": {
        "id": "sDXjfzFMNkP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sad happy\n",
        "\n",
        "emoticons_sad_happy= set(['◉‿◉','o ^ - ^ o','^_^',':)',':^)',\n",
        "\n",
        "': NS)','8)',':HAI','@',':HAI)',':‑)',\n",
        "\n",
        "':P',':@','8‑D',':‑)',':-&','=D',':‑p',\n",
        "\n",
        "'=‑D','XD',':D',':-))','%-)',':}',\n",
        "\n",
        "':c)',':^)',':C','>:3',':>','L',':-/','>:/',\n",
        "\n",
        "':S','>:[','@',':-(','\"[\"',':-<',\n",
        "\n",
        "'=\\\\','=/','>:(',':',':{','>:\\\\',\n",
        "\n",
        "'(',':','(\", \")',':','(\"',':-c',\n",
        "\n",
        "'> :(',':‑',':NS('])\n",
        "\n",
        "# all emot happy + sad\n",
        "\n",
        "emoticons = emoticons_phone.union(emoticons_sad_happy)"
      ],
      "metadata": {
        "id": "QUXOoetPNvmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fulltext(tweet):\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    #remove coma\n",
        "    tweet = re.sub(r',','',tweet)\n",
        "    #remove angka\n",
        "    tweet = re.sub('[0-9]+', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False,\n",
        "    strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_indonesia and # remove stopwords\n",
        "            word not in emoticons and # remove emoticons\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "    #tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "            return tweets_clean"
      ],
      "metadata": {
        "id": "AeFB_APvNwtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['token'] = df['h_angka'].apply(lambda x: fulltext(x))"
      ],
      "metadata": {
        "id": "G2uNVO576s-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "sIyBJvCw7HvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punct\n",
        "def remove_punct(text):\n",
        "    text = \" \".join([char for char in text if char not in string.punctuation])\n",
        "    return text"
      ],
      "metadata": {
        "id": "TX9C39hxlorL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simpan di kolom tabel baru tweet\n",
        "df['fulltext'] = df['text'].apply(lambda x: remove_punct(x))"
      ],
      "metadata": {
        "id": "C8k8yAFroZb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "4THa0I2foZfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mengurutkan ascending urutkan kolom tweet\n",
        "df.sort_values(\"fulltext\", inplace = True)"
      ],
      "metadata": {
        "id": "ETI8pOE_ot92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0kqwuvnUouBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "B-reVs8vo0ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_transformer = CountVectorizer().fit(df['fulltext'])\n",
        "bow_transformer.vocabulary_"
      ],
      "metadata": {
        "id": "Qo-9u16Io0ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = bow_transformer.get_feature_names_out()\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "E4QWTE-vtWKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_bow = bow_transformer.transform(df['text'])\n",
        "print(text_bow)\n",
        "if teks is not None:\n",
        "    teks_kecil = teks.lower()\n",
        "else:\n",
        "    print(\"Variabel 'teks' tidak memiliki nilai.\")\n"
      ],
      "metadata": {
        "id": "Qf32MNFNpNos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = text_bow.toarray()\n",
        "print(X)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "SwWyfax-pNss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer=TfidfTransformer().fit(text_bow)\n",
        "print(tfidf_transformer)\n",
        "\n",
        "tweet_tfidf=tfidf_transformer.transform(text_bow)\n",
        "print(tweet_tfidf)\n",
        "print(tweet_tfidf.shape)"
      ],
      "metadata": {
        "id": "OC0GO6sMumUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, df.label,test_size=0.2, random_state=35)"
      ],
      "metadata": {
        "id": "9FeyL1ag3c7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB().fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "_y75zImR3dLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(x_test)\n",
        "predict= pd.Series(prediction)\n",
        "print(predict.to_string())"
      ],
      "metadata": {
        "id": "eqaefugk33YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_label= pd.Series(y_test)\n",
        "print(true_label.to_string())"
      ],
      "metadata": {
        "id": "7Bdl69rs33pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "t = time()\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "test_time = time() - t\n",
        "print(\"test time:  %0.3fs\" % test_time)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy:   %0.3f\" % score1)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred, target_names=['negatif', 'netral', 'positif']))\n",
        "\n",
        "columns = ['negatif','netral','positif']\n",
        "confm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = DataFrame(confm, index=columns, columns=columns)\n",
        "\n",
        "ax = sn.heatmap(df_cm, cmap='Greens', annot=True)\n",
        "ax.set_title('Confusion matrix')\n",
        "ax.set_xlabel('Label prediksi')"
      ],
      "metadata": {
        "id": "ak4Op-T04IRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, x_train,y_train, cv=10)\n",
        "\n",
        "# Print the accuracy of each fold:\n",
        "print(scores)\n",
        "\n",
        "# Print the mean accuracy of all 10 folds\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "Equ0vEXq4QwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_data = [\"alhamdulillah vaksin apapun itu aman dan baik bagi tubuh kita\",\n",
        "             \"vaksin hanyalah bisnis pemerintah dan konspirasi elit global\",\n",
        "             \"besok saya dan ibu vaksin di jakarta selatan\"]\n",
        "print(user_data)"
      ],
      "metadata": {
        "id": "G9Ys-VpJ4Q3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_1_unseen =  bow_transformer.transform(user_data)\n",
        "data=test_1_unseen.toarray()\n",
        "print(\"diubah menjadi array: \\n \",data)"
      ],
      "metadata": {
        "id": "UOA1LB7JU0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ukuran dari array: \\n \",data.shape)"
      ],
      "metadata": {
        "id": "B5rUg9gCU0q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_unseen = model.predict(data)\n",
        "print(prediction_unseen)"
      ],
      "metadata": {
        "id": "etpXLUTVU5Ce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}